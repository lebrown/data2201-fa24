{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "374b42b7-3087-4b6a-95b3-315feb64767b",
   "metadata": {},
   "source": [
    "# Lecture 26\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e7fd03-0858-406d-9d56-c831e63bcd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d17a42-8459-40df-a9b3-d8c8d48bc641",
   "metadata": {},
   "source": [
    "In this lecture, we will work with the `vehicles` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d390cfe-927b-43d9-982b-93e149f2d31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles = sns.load_dataset(\"mpg\").rename(columns={\"horsepower\":\"hp\"}).dropna().sort_values(\"hp\")\n",
    "vehicles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86aff740-7850-452d-813f-4e3afff7e320",
   "metadata": {},
   "source": [
    "We will attempt to predict a car's `\"mpg\"` from transformations of its `\"hp\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bedda4-ee08-437c-87ad-51442df5d864",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vehicles[[\"hp\"]]\n",
    "X[\"hp^2\"] = vehicles[\"hp\"]**2\n",
    "X[\"hp^3\"] = vehicles[\"hp\"]**3\n",
    "X[\"hp^4\"] = vehicles[\"hp\"]**4\n",
    "\n",
    "Y = vehicles[\"mpg\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf51354-d57b-4b14-8148-125168b6a82c",
   "metadata": {},
   "source": [
    "## Test Sets\n",
    "\n",
    "To perform a train-test split, we can use the `train_test_split` function of the `sklearn.model_selection` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d140790-b5cb-46b5-aedd-d775fdda2513",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# `test_size` specifies the proportion of the full dataset that should be allocated to testing.\n",
    "# `random_state` makes our results reproducible for educational purposes.\n",
    "# shuffle is True by default and randomizes the data before splitting.\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=100, \n",
    "                                                    shuffle=True)\n",
    "\n",
    "print(f\"Size of full dataset: {X.shape[0]} points\")\n",
    "print(f\"Size of training set: {X_train.shape[0]} points\")\n",
    "print(f\"Size of test set: {X_test.shape[0]} points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece2c817-5240-472c-9dfb-6fb6980dcf5e",
   "metadata": {},
   "source": [
    "We then fit the model using the training set..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56abfad-f50f-4903-a52a-6f4bb6f40362",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.linear_model as lm\n",
    "\n",
    "model = lm.LinearRegression()\n",
    "\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ff05ad-a1d1-4d0a-a1c3-4457f1b431cf",
   "metadata": {},
   "source": [
    "...and evaluate its performance by making predictions on the test set. Notice that the model performs more poorly on the test data it did not encounter during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c744c410-b922-4f9f-ac1c-76e47a1e56c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "train_error = mean_squared_error(Y_train, model.predict(X_train))\n",
    "test_error = mean_squared_error(Y_test, model.predict(X_test))\n",
    "\n",
    "print(f\"Training error: {train_error}\")\n",
    "print(f\"Test error: {test_error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663f61e7-97b8-486d-bd1d-6344f0cc2a72",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Validation Sets\n",
    "\n",
    "To assess model performance on unseen data, then *use* this information to finetune the model, we introduce a validation set. You can imagine this as us splitting the training set into a validation set and a \"mini\" training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac3eea7-c220-4838-bc04-bb0b06ca4458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split X_train further into X_train_mini and X_val.\n",
    "X_train_mini, X_val, Y_train_mini, Y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=100)\n",
    "\n",
    "print(f\"Size of original training set: {X_train.shape[0]} points\")\n",
    "print(f\"Size of mini training set: {X_train_mini.shape[0]} points\")\n",
    "print(f\"Size of validation set: {X_val.shape[0]} points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27160655-4f96-4925-afde-647b66411318",
   "metadata": {},
   "source": [
    "In the cell below, we repeat the experiment from Lecture 14: fit several models of increasing complexity, then compute their errors. Here, we find the model's errors on the **validation set** to understand how model complexity influences performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872ba3ec-b6ec-4aea-a8b7-2e1f156ea579",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, dpi=200, figsize=(12, 3))\n",
    "\n",
    "for order in [2, 3, 4]:\n",
    "    model = lm.LinearRegression()\n",
    "    model.fit(X_train_mini.iloc[:, :order], Y_train_mini)\n",
    "    val_predictions = model.predict(X_val.iloc[:, :order])\n",
    "    \n",
    "    output = X_val.iloc[:, :order]\n",
    "    output[\"y_hat\"] = val_predictions\n",
    "    output = output.sort_values(\"hp\")\n",
    "    \n",
    "    ax[order-2].scatter(X_val[\"hp\"], Y_val, edgecolor=\"white\", lw=0.5)\n",
    "    ax[order-2].plot(output[\"hp\"], output[\"y_hat\"], \"tab:red\")\n",
    "    ax[order-2].set_title(f\"Model with degree {order}\")\n",
    "    ax[order-2].set_xlabel(\"hp\")\n",
    "    ax[order-2].set_ylabel(\"mpg\")\n",
    "    ax[order-2].annotate(f\"Validation MSE: {np.round(mean_squared_error(Y_val, val_predictions), 3)}\", (90, 30))\n",
    "\n",
    "plt.subplots_adjust(wspace=0.3);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09947a6f-a2df-46d5-8549-949b58ae12a6",
   "metadata": {},
   "source": [
    "Let's repeat this process:\n",
    "\n",
    "1. Fit an degree-x model to the mini training set\n",
    "2. Evaluate the fitted model's MSE when making predictions on the validation set\n",
    "\n",
    "We use the model's performance on the validation set as a guide to selecting the best combination of features. We are not limited in the number of times we use the validation set – we just never use this set to fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dfd6b9-85de-44ee-9c82-72f29e4e01bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "def fit_model_dataset(degree):\n",
    "    pipelined_model = Pipeline([\n",
    "            ('polynomial_transformation', PolynomialFeatures(degree)),\n",
    "            ('linear_regression', lm.LinearRegression())    \n",
    "        ])\n",
    "\n",
    "    pipelined_model.fit(X_train_mini[[\"hp\"]], Y_train_mini)\n",
    "    return mean_squared_error(Y_val, pipelined_model.predict(X_val[[\"hp\"]]))\n",
    "\n",
    "errors = [fit_model_dataset(degree) for degree in range(0, 18)]\n",
    "MSEs_and_k = pd.DataFrame({\"k\": range(0, 18), \"MSE\": errors})\n",
    "\n",
    "plt.figure(dpi=120)\n",
    "plt.plot(range(0, 18), errors)\n",
    "plt.xlabel(\"Model Complexity (degree of polynomial)\")\n",
    "plt.ylabel(\"Validation MSE\")\n",
    "plt.xticks(range(0, 18));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57b3792-f663-4fcf-b62e-de4d22a72898",
   "metadata": {},
   "outputs": [],
   "source": [
    "MSEs_and_k.rename(columns={\"k\":\"Degree\"}).set_index(\"Degree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3747069-c499-44b8-93a0-b30b1e6f0323",
   "metadata": {},
   "source": [
    "From this **model selection** process, we might choose to create a model with degree 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4933662b-f912-4e5c-b86a-449c15135e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Polynomial degree with lowest validation error: {MSEs_and_k.sort_values(\"MSE\").head(1)[\"k\"].values}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa39fe5-a467-44e5-8f2d-0948210c3a28",
   "metadata": {},
   "source": [
    "After this choice has been finalized, and we are completely finished with the model design process, we finally assess model performance on the test set. We typically use the entire training set (both the \"mini\" training set and validation set) to fit the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd0937d-23d1-465e-95bd-8d878a1c56fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update our training and test sets to include all polynomial features between 5 and 9\n",
    "for degree in range(5, 9):\n",
    "    X_train[f\"hp^{degree}\"] = X_train[\"hp\"]**degree\n",
    "    X_test[f\"hp^{degree}\"] = X_test[\"hp\"]**degree\n",
    "\n",
    "final_model = lm.LinearRegression()\n",
    "final_model.fit(X_train, Y_train)\n",
    "\n",
    "print(f\"Test MSE of final model: {mean_squared_error(Y_test, final_model.predict(X_test))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82e44e7-e3a9-46d0-af00-9424216738d3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Cross-Validation\n",
    "\n",
    "The validation set gave us an opportunity to understand how the model performs on a **single** set of unseen data. The specific validation set we drew was fixed – we used the same validation points every time.\n",
    "\n",
    "It's possible that we may have, by random chance, selected a set of validation points that was *not* representative of other unseen data that the model might encounter (for example, if we happened to have selected all outlying data points for the validation set).\n",
    "\n",
    "Different train/validation splits lead to different validation errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c958576b-9488-4c7b-b741-38c3baa85e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 4):\n",
    "    X_train_mini, X_val, Y_train_mini, Y_val = train_test_split(X_train, Y_train, test_size=0.2)\n",
    "    model = lm.LinearRegression()\n",
    "    model.fit(X_train_mini, Y_train_mini)\n",
    "    y_hat = model.predict(X_val)\n",
    "    print(f\"Val error from train/validation split #{i}: {mean_squared_error(y_hat, Y_val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b8725b-8863-4341-8641-0ae021f8ceba",
   "metadata": {},
   "source": [
    "To apply cross-validation, we use the `KFold` class of `sklearn.model_selection`. `KFold` will return the indices of each cross-validation fold. Then, we iterate over each of these folds to designate it as the validation set, while training the model on the remaining folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a577efc-ecd0-4757-854b-8d9c03e733f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "np.random.seed(25) # Ensures reproducibility of this notebook\n",
    "\n",
    "# n_splits sets the number of folds to create\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "validation_errors = []\n",
    "\n",
    "for train_idx, valid_idx in kf.split(X_train):\n",
    "    # Split the data\n",
    "    split_X_train, split_X_valid = X_train.iloc[train_idx], X_train.iloc[valid_idx]\n",
    "    split_Y_train, split_Y_valid = Y_train.iloc[train_idx], Y_train.iloc[valid_idx]\n",
    "\n",
    "    # Fit the model on the training split\n",
    "    model.fit(split_X_train, split_Y_train)\n",
    "\n",
    "    error = mean_squared_error(model.predict(split_X_valid), split_Y_valid)\n",
    "\n",
    "    validation_errors.append(error)\n",
    "\n",
    "print(f\"Cross-validation error: {np.mean(validation_errors)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:data1202] *",
   "language": "python",
   "name": "conda-env-data1202-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
