{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import warnings\n",
    "pd.options.mode.chained_assignment = None \n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "<hr style=\"border: 5px solid #8a8c8c;\" />\n",
    "<hr style=\"border: 1px solid #ffcd00;\" />\n",
    "\n",
    "## Using scikit-learn to fit our Multiple Linear Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the `penguins` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sns.load_dataset(\"penguins\")\n",
    "df = df[df[\"species\"] == \"Adelie\"].dropna()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we could measure flippers and weight easily, but not bill dimensions.\n",
    "How can we predict **bill depth** from flipper length and/or body mass?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For demo purposes, we'll drop all columns except the variables whose relationships we're interested in modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sns.load_dataset(\"penguins\")\n",
    "df = df[df[\"species\"] == \"Adelie\"].dropna()\n",
    "df = df[[\"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to create a linear regression model that predicts a penguin's **bill depth** $y$ using both their **flipper length** $x_1$ and **body mass** $x_2$, plus an intercept term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\Large \\hat{y} = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLS Approach 1: Use Solution to Normal Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw in last lecture that we can model the above multiple linear regression equation using matrix multiplication:\n",
    "\n",
    "$$ \\large \\hat{\\mathbb{Y}} = \\mathbb{X} \\theta$$\n",
    "\n",
    "The optimal $\\hat{\\theta}$ that minimizes MSE also solves the **normal equation**:\n",
    "\n",
    "$$ \\left( \\mathbb{X}^T \\mathbb{X} \\right) \\hat{\\theta} = \\mathbb{X}^T \\mathbb{Y}$$\n",
    "\n",
    "If $\\mathbb{X}$ is full column rank, then there is a unique solution to $\\hat{\\theta}$\n",
    "\n",
    "$$\\large \\hat{\\theta} = \\left( \\mathbb{X}^T \\mathbb{X} \\right)^{-1} \\mathbb{X}^T \\mathbb{Y}$$\n",
    "\n",
    "Note that this derivation is one of the most challenging in the course, especially for those of you who are learning linear algebra during the same semester."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "Let's try this in code. We will add a bias term both so we actually represent the linear equation we proposed (with intercept), as well as so that our residuals sum to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[[\"flipper_length_mm\", \"body_mass_g\"]]\n",
    "X[\"bias\"] = 1\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"bill_depth_mm\"]\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the solution to the normal equation if $\\mathbb{X}$ is full column rank:\n",
    "\n",
    "$$\\hat{\\theta} = \\left( \\mathbb{X}^T \\mathbb{X} \\right)^{-1} \\mathbb{X}^T \\mathbb{Y}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.inv(X.T @ X) @ X.T @ y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_using_normal_equation =  np.linalg.inv(X.T @ X) @ X.T @ y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The code above is inefficient. We won't go into this in our class, but it's better to use `np.linalg.solve` rather than computing the explicit matrix inverse.\n",
    "\n",
    "This also doesn't work if our X is **not invertible**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make Predictions\n",
    "We'll save the predictions in a column so we can compare them against using sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"pred_bill_depth_mm\"] = X.to_numpy() @ theta_using_normal_equation\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "\n",
    "---\n",
    "\n",
    "### Using sklearn to fit our Multiple Linear Regression Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternate approach to optimize our model is to use the `sklearn.linear_model.LinearRegression` class. [(Documentation)](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Create an `sklearn` object.**\n",
    "\n",
    "    First we create a model. At this point the model has not been fit so it has no parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **`fit` the object to data.**\n",
    "\n",
    "    Then we \"fit\" the model, which means computing the parameters that minimize the loss function. The `LinearRegression` class is hard coded to use the **MSE** as its loss function. The first argument of the fit function should be a matrix (or DataFrame), and the second should be the observations we're trying to predict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    X=df[[\"flipper_length_mm\", \"body_mass_g\"]], \n",
    "    y=df[\"bill_depth_mm\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Analyze fit or call `predict`.**\n",
    "\n",
    "    Now that our model is trained, we can ask it questions. The code below asks the model to estimate the bill depth (in mm) for a penguin with a 185 mm flipper length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict([[185, 3750.0]]) # why the double brackets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also ask the model to generate a series of predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sklearn_preds\"] = model.predict(df[[\"flipper_length_mm\", \"body_mass_g\"]])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the predictions, we see that they are exactly the same as we got using our analytic formula!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyze parameters.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can ask the model for its intercept and slope with `_intercept` and `_coef`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.intercept_      # why is this a scalar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.coef_           # why is this an array?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " They are the same as with our analytic formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vs. analytical solutions\n",
    "theta_using_normal_equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They look the same, but why are they out of order...?\n",
    "\n",
    "Remember our order of columns used for normal equation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Fit\n",
    "We can visualize this data in three dimensions but for many (most) real-world problems this will not be possible (or helpful).\n",
    "\n",
    "Note, the following code is out of scope for this class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(df, x=\"flipper_length_mm\", y=\"body_mass_g\", z=\"bill_depth_mm\")\n",
    "\n",
    "# Create a grid of points to evaluate plane\n",
    "grid_resolution = 2\n",
    "(u,v) = np.meshgrid(\n",
    "    np.linspace(df[\"flipper_length_mm\"].min(), df[\"flipper_length_mm\"].max(), grid_resolution),\n",
    "    np.linspace(df[\"body_mass_g\"].min(), df[\"body_mass_g\"].max(), grid_resolution))\n",
    "features = pd.DataFrame({\"flipper_length_mm\": u.flatten(),\n",
    "                         \"body_mass_g\": v.flatten()})\n",
    "# Make predictions at every point on the grid\n",
    "zs = model.predict(features)\n",
    "\n",
    "# create the Surface\n",
    "fig.add_trace(go.Surface(x=u, y=v, z= zs.reshape(u.shape), opacity=0.9, showscale=False))\n",
    "fig.update_layout(autosize=False, width=800, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the predictions all lie in a plane. In higher dimensions, they all lie in a \"hyperplane\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyze performance.**\n",
    "\n",
    "The `sklearn` package also provides a function that computes the MSE from a list of observations and predictions. This avoids us having to manually compute MSE by first computing residuals.\n",
    "\n",
    "[documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import  mean_squared_error\n",
    "mean_squared_error(df[\"bill_depth_mm\"], df[\"sklearn_preds\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "--- \n",
    "\n",
    "**Return to Lecture**, Slide 8\n",
    "\n",
    "--- \n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "<hr style=\"border: 5px solid #8a8c8c;\" />\n",
    "<hr style=\"border: 1px solid #ffcd00;\" />\n",
    "\n",
    "## Feature Engineering\n",
    "\n",
    "Feature engineering is the process of applying **feature functions** to generate new features for use in modeling. In this lecture, we will discuss:\n",
    "* One-hot encoding\n",
    "* Polynomial features\n",
    "\n",
    "To do so, we'll work with our familiar `tips` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips = sns.load_dataset(\"tips\")\n",
    "tips.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### One-Hot Encoding\n",
    "\n",
    "One-hot encoding is a feature engineering technique to generate numeric feature from categorical data. For example, we can use one-hot encoding to incorporate the day of the week as an input into a regression model.\n",
    "\n",
    "![](ohe.png)\n",
    "\n",
    "Suppose we want to use a design matrix of three features – the `total_bill`, `size`, and `day` – to predict the tip. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_raw = tips[[\"total_bill\", \"size\", \"day\"]]\n",
    "y = tips[\"tip\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because `day` is non-numeric, we will apply one-hot encoding before fitting a model.\n",
    "\n",
    "The `OneHotEncoder` class of `sklearn` ([documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder.get_feature_names_out)) offers a quick way to perform one-hot encoding. You will explore its use in detail in Lab 7. For now, recognize that we follow a very similar workflow to when we were working with the `LinearRegression` class: we initialize a `OneHotEncoder` object, fit it to our data, then use `.transform` to apply the fitted encoder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Initialize a OneHotEncoder object\n",
    "ohe = OneHotEncoder()\n",
    "\n",
    "# Fit the encoder\n",
    "ohe.fit(tips[[\"day\"]])\n",
    "\n",
    "# Use the encoder to transform the raw \"day\" feature\n",
    "encoded_day = ohe.transform(tips[[\"day\"]]).toarray()\n",
    "encoded_day_df = pd.DataFrame(encoded_day, columns=ohe.get_feature_names_out())\n",
    "\n",
    "encoded_day_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `OneHotEncoder` has converted the categorical `day` feature into four numeric features! Recall that the `tips` dataset only included data for Thursday through Sunday, which is why only four days of the week appear. \n",
    "\n",
    "Let's join this one-hot encoding to the original data to form our featurized design matrix. We drop the original `day` column so our design matrix only includes numeric values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_raw.join(encoded_day_df).drop(columns=\"day\")\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use `sklearn`'s `LinearRegression` class to fit a model to this design matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "ohe_model = LinearRegression(fit_intercept=False) # Tell sklearn to not add an additional bias column. Why?\n",
    "ohe_model.fit(X, y)\n",
    "\n",
    "pd.DataFrame({\"Feature\":X.columns, \"Model Coefficient\":ohe_model.coef_}).set_index(\"Feature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Bonus: Advanced Feature Engineering Pipelines in Scikit Learn\n",
    "You can use the scikit learn `Pipeline` class to define sophisticated feature engineering pipelines that combine different transformations that are all fit at once.\n",
    "\n",
    "A pipeline is a sequence of transformations and models that are composed to form a new model. It is implemented as a list of  (\"name\", model/transformation) tuples.  Here we have a feature engineering stage followed by a linear model. The feature engineering stage uses a `ColumnTransformer` that applies named transformations to selected columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "pl = Pipeline([\n",
    "    (\"Feature Engineering\", \n",
    "     ColumnTransformer(  \n",
    "         [(\"OHE\", OneHotEncoder(), [\"day\"]),\n",
    "         (\"Polynomial\", PolynomialFeatures(4), ['total_bill'])], \n",
    "        remainder=\"passthrough\")), \n",
    "    (\"Linear Model\", LinearRegression()) # Linear Model\n",
    "]) \n",
    "pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.fit(X_raw, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl['Feature Engineering'].get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips[\"predicted\"] = pl.predict(X_raw)\n",
    "tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Return to Lecture, Slide 19\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "<br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Polynomial Features\n",
    "\n",
    "Consider the `vehicles` dataset, which includes information about cars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles = sns.load_dataset(\"mpg\").dropna().rename(columns = {\"horsepower\": \"hp\"}).sort_values(\"hp\")\n",
    "vehicles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to use the `hp` (horsepower) of a car to predict its `mpg` (gas mileage in miles per gallon). If we visualize the relationship between these two variables, we see a non-linear curvature. Fitting a linear model to these variables results in a high (poor) value of RMSE. \n",
    "\n",
    "$$\\hat{y} = \\theta_0 + \\theta_1 (\\text{hp})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vehicles[[\"hp\"]]\n",
    "y = vehicles[\"mpg\"]\n",
    "\n",
    "hp_model = LinearRegression()\n",
    "hp_model.fit(X, y)\n",
    "hp_model_predictions = hp_model.predict(X)\n",
    "\n",
    "print(f\"MSE of model with (hp) feature: {np.mean((y-hp_model_predictions)**2)}\")\n",
    "\n",
    "fig = px.scatter(vehicles, x=\"hp\", y=\"mpg\", width=800, height=600)\n",
    "fig.add_trace(go.Scatter(x=vehicles[\"hp\"], y=hp_model_predictions,\n",
    "                         mode=\"lines\", name=\"Linear Prediction\"))\n",
    "# sns.scatterplot(data=vehicles, x=\"hp\", y=\"mpg\")\n",
    "# plt.plot(vehicles[\"hp\"], hp_model_predictions, c=\"tab:red\");\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To capture the non-linear relationship between the variables, we can introduce a non-linear feature: `hp` squared. Our new model is:\n",
    "\n",
    "$$\\hat{y} = \\theta_0 + \\theta_1 (\\text{hp}) + \\theta_2 (\\text{hp}^2)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vehicles[[\"hp\"]]\n",
    "X.loc[:, \"hp^2\"] = vehicles[\"hp\"]**2\n",
    "\n",
    "hp2_model = LinearRegression()\n",
    "hp2_model.fit(X, y)\n",
    "hp2_model_predictions = hp2_model.predict(X)\n",
    "\n",
    "\n",
    "print(f\"MSE of model with (hp^2) feature: {np.mean((y-hp2_model_predictions)**2)}\")\n",
    "\n",
    "fig = px.scatter(vehicles, x=\"hp\", y=\"mpg\", width=800, height=600)\n",
    "fig.add_trace(go.Scatter(x=vehicles[\"hp\"], y=hp2_model_predictions,\n",
    "                         mode=\"lines\", name=\"Quadratic Prediction\"))\n",
    "# sns.scatterplot(data=vehicles, x=\"hp\", y=\"mpg\")\n",
    "# plt.plot(vehicles[\"hp\"], hp2_model_predictions, c=\"tab:red\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we take things further and add even *more* polynomial features?\n",
    "\n",
    "The cell below fits models of increasing complexity and computes their MSEs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(predictions, observations):\n",
    "    return np.mean((observations - predictions)**2)\n",
    "\n",
    "# Add hp^3 and hp^4 as features to the data\n",
    "X[\"hp^3\"] = vehicles[\"hp\"]**3\n",
    "X[\"hp^4\"] = vehicles[\"hp\"]**4\n",
    "\n",
    "# Fit a model with order 3\n",
    "hp3_model = LinearRegression()\n",
    "hp3_model.fit(X[[\"hp\", \"hp^2\", \"hp^3\"]], vehicles[\"mpg\"])\n",
    "hp3_model_predictions = hp3_model.predict(X[[\"hp\", \"hp^2\", \"hp^3\"]])\n",
    "\n",
    "# Fit a model with order 4\n",
    "hp4_model = LinearRegression()\n",
    "hp4_model.fit(X[[\"hp\", \"hp^2\", \"hp^3\", \"hp^4\"]], vehicles[\"mpg\"])\n",
    "hp4_model_predictions = hp4_model.predict(X[[\"hp\", \"hp^2\", \"hp^3\", \"hp^4\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(vehicles, x=\"hp\", y=\"mpg\", width=800, height=600)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=vehicles[\"hp\"], y=hp_model_predictions, mode=\"lines\", \n",
    "    name=f\"HP MSE={np.round(mse(vehicles['mpg'], hp_model_predictions),3)}\"))\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=vehicles[\"hp\"], y=hp2_model_predictions, mode=\"lines\", \n",
    "    name=f\"HP^2 MSE={np.round(mse(vehicles['mpg'], hp2_model_predictions),3)}\"))\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=vehicles[\"hp\"], y=hp3_model_predictions, mode=\"lines\", \n",
    "    name=f\"HP^3 MSE={np.round(mse(vehicles['mpg'], hp3_model_predictions),3)}\"))\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=vehicles[\"hp\"], y=hp4_model_predictions, mode=\"lines\", \n",
    "    name=f\"HP^4 MSE={np.round(mse(vehicles['mpg'], hp4_model_predictions),3)}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing differently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the models' predictions\n",
    "fig, ax = plt.subplots(1, 3, dpi=200, figsize=(12, 3))\n",
    "\n",
    "predictions_dict = {0:hp2_model_predictions, 1:hp3_model_predictions, 2:hp4_model_predictions}\n",
    "\n",
    "for i in predictions_dict:\n",
    "    ax[i].scatter(vehicles[\"hp\"], vehicles[\"mpg\"], edgecolor=\"white\", lw=0.5)\n",
    "    ax[i].plot(vehicles[\"hp\"], predictions_dict[i], \"tab:red\")\n",
    "    ax[i].set_title(f\"Model with order {i+2}\")\n",
    "    ax[i].set_xlabel(\"hp\")\n",
    "    ax[i].set_ylabel(\"mpg\")\n",
    "    ax[i].annotate(f\"MSE: {np.round(mse(vehicles['mpg'], predictions_dict[i]), 3)}\", (120, 40))\n",
    "\n",
    "plt.subplots_adjust(wspace=0.3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "Return to Slides, Slide 27\n",
    "\n",
    "---\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "## Complexity and Overfitting\n",
    "\n",
    "What we saw above was the phenomenon of **model complexity** – as we add additional features to the design matrix, the model becomes increasingly *complex*. Models with higher complexity have lower values of training error. Intuitively, this makes sense: with more features at its disposal, the model can match the observations in the trainining data more and more closely. \n",
    "\n",
    "We can run an experiment to see this in action. In the cell below, we fit many models of progressively higher complexity, then plot the MSE of predictions on the training set. The code used (specifically, the `Pipeline` and `PolynomialFeatures` functions of `sklearn`) is out of scope.\n",
    "\n",
    "The **order** of a polynomial model is the highest power of any term in the model. An order 0 model takes the form $\\hat{y} = \\theta_0$, while an order 4 model takes the form $\\hat{y} = \\theta_0 + \\theta_1 x + \\theta_2 x^2 + \\theta_3 x^3 + \\theta_4 x^4$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "def fit_model_dataset(degree, dataset):\n",
    "    pipelined_model = Pipeline([\n",
    "            ('polynomial_transformation', PolynomialFeatures(degree)),\n",
    "            ('linear_regression', LinearRegression())    \n",
    "        ])\n",
    "\n",
    "    pipelined_model.fit(dataset[[\"hp\"]], dataset[\"mpg\"])\n",
    "    return mse(dataset['mpg'], pipelined_model.predict(dataset[[\"hp\"]]))\n",
    "\n",
    "errors = [fit_model_dataset(degree, vehicles) for degree in range(0, 8)]\n",
    "MSEs_and_k = pd.DataFrame({\"k\": range(0, 8), \"MSE\": errors})\n",
    "\n",
    "plt.plot(range(0, 8), errors)\n",
    "plt.xlabel(\"Model Complexity (degree of polynomial)\")\n",
    "plt.ylabel(\"Training MSE\");\n",
    "\n",
    "def plot_degree_k_model(k, MSEs_and_k, axs):\n",
    "    pipelined_model = Pipeline([\n",
    "        ('poly_transform', PolynomialFeatures(degree = k)),\n",
    "        ('regression', LinearRegression(fit_intercept = True))    \n",
    "    ])\n",
    "    pipelined_model.fit(vehicles[[\"hp\"]], vehicles[\"mpg\"])\n",
    "    \n",
    "    row = k // 4\n",
    "    col = k % 4\n",
    "    ax = axs[row, col]\n",
    "    \n",
    "    sns.scatterplot(data=vehicles, x='hp', y='mpg', ax=ax)\n",
    "    \n",
    "    x_range = np.linspace(45, 210, 100).reshape(-1, 1)\n",
    "    ax.plot(x_range, pipelined_model.predict(pd.DataFrame(x_range, columns=['hp'])), c='tab:red', linewidth=2)\n",
    "    \n",
    "    ax.set_ylim((0, 50))\n",
    "    mse_str = f\"MSE: {MSEs_and_k.loc[k, 'MSE']:.4}\\nDegree: {k}\"\n",
    "    ax.text(130, 35, mse_str, dict(size=14))\n",
    "\n",
    "fig = plt.figure(figsize=(15, 6), dpi=150)\n",
    "axs = fig.subplots(nrows=2, ncols=4)\n",
    "\n",
    "for k in range(8):\n",
    "    plot_degree_k_model(k, MSEs_and_k, axs)\n",
    "fig.subplots_adjust(wspace=0.4, hspace=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the model increases in polynomial degree (that is, it increases in complexity), the training MSE decreases, plateauing at roughly ~18.\n",
    "\n",
    "In fact, it is a mathematical fact that if we create a polynomial model with degree $n-1$, we can *perfectly* model a set of $n$ points. For example, a set of 5 points can be perfectly modeled by a degree 4 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(101)\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, dpi=200, figsize=(12, 3))\n",
    "\n",
    "for i in range(0, 3):\n",
    "    points = 3*np.random.uniform(size=(5, 2))\n",
    "\n",
    "    polynomial_model = Pipeline([\n",
    "                ('polynomial_transformation', PolynomialFeatures(4)),\n",
    "                ('linear_regression', LinearRegression())    \n",
    "            ])\n",
    "\n",
    "    polynomial_model.fit(points[:, [0]], points[:, 1])\n",
    "\n",
    "    ax[i].scatter(points[:, 0], points[:, 1])\n",
    "\n",
    "    xs = np.linspace(0, 3)\n",
    "    ax[i].plot(xs, polynomial_model.predict(xs[:, np.newaxis]), c=\"tab:red\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may be tempted to always design models with high polynomial degree – after all, we know that we could theoretically achieve perfect predictions by creating a model with enough polynomial features. \n",
    "\n",
    "It turns out that the examples we looked at above represent a somewhat artificial scenario: we trained our model on all the data we had available, then used the model to make predictions on this very same dataset. A more realistic situation is when we wish to apply our model on unseen data – that is, datapoints that it did not encounter during the model fitting process. \n",
    "\n",
    "Suppose we obtain a random sample of 6 datapoints from our population of vehicle data. We want to train a model on these 6 points and use it to make predictions on unseen data (perhaps cars for which we don't already know the true `mpg`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "\n",
    "sample_6 = vehicles.sample(6)\n",
    "\n",
    "sns.scatterplot(data=sample_6, x=\"hp\", y=\"mpg\")\n",
    "plt.ylim(-35, 50)\n",
    "plt.xlim(0, 250);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we design a model with polynomial degree 5, we can make perfect predictions on this sample of training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_5_model = Pipeline([\n",
    "                ('polynomial_transformation', PolynomialFeatures(5)),\n",
    "                ('linear_regression', LinearRegression())    \n",
    "            ])\n",
    "\n",
    "degree_5_model.fit(sample_6[[\"hp\"]], sample_6[\"mpg\"])\n",
    "xs = np.linspace(0, 250, 1000)\n",
    "degree_5_model_predictions = degree_5_model.predict(xs[:, np.newaxis])\n",
    "\n",
    "plt.plot(xs, degree_5_model_predictions, c=\"tab:red\")\n",
    "sns.scatterplot(data=sample_6, x=\"hp\", y=\"mpg\", s=50)\n",
    "plt.ylim(-35, 50)\n",
    "plt.xlim(0, 250);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, when we reapply this fitted model to the full population of data, it fails to capture the major trends of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(xs, degree_5_model_predictions, c=\"tab:red\")\n",
    "sns.scatterplot(data=vehicles, x=\"hp\", y=\"mpg\", s=50)\n",
    "plt.ylim(-35, 50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has **overfit** to the data used to train it. It has essentially \"memorized\" the six datapoints used during model fitting, and does not generalize well to new data. \n",
    "\n",
    "Complex models tend to be more sensitive to the data used to train them. The **variance** of a model refers to its tendency to vary depending on the training data used during model fitting. It turns out that our degree-5 model has very high model variance. If we randomly sample new sets of datapoints to use in training, the model varies erratically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, dpi=200, figsize=(12, 3))\n",
    "\n",
    "for i in range(0, 3):\n",
    "    sample = vehicles.sample(6)\n",
    "\n",
    "    polynomial_model = Pipeline([\n",
    "                ('polynomial_transformation', PolynomialFeatures(5)),\n",
    "                ('linear_regression', LinearRegression())    \n",
    "            ])\n",
    "\n",
    "    polynomial_model.fit(sample[[\"hp\"]], sample[\"mpg\"])\n",
    "\n",
    "    ax[i].scatter(sample[[\"hp\"]], sample[\"mpg\"])\n",
    "\n",
    "    xs = np.linspace(50, 210, 1000)\n",
    "    ax[i].plot(xs, polynomial_model.predict(xs[:, np.newaxis]), c=\"tab:red\")\n",
    "    ax[i].set_ylim(-80, 100)\n",
    "    ax[i].set_xlabel(\"hp\")\n",
    "    ax[i].set_ylabel(\"mpg\")\n",
    "    ax[i].set_title(f\"Resample #{i+1}\")\n",
    "    \n",
    "fig.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing differently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(16)\n",
    "\n",
    "fig = px.scatter(vehicles, x=\"hp\", y=\"mpg\", width=800, height=600)\n",
    "\n",
    "for i in range(0, 3):\n",
    "    sample = vehicles.sample(6).sort_values(\"hp\")\n",
    "    \n",
    "    polynomial_model = Pipeline([\n",
    "                ('polynomial_transformation', PolynomialFeatures(5)),\n",
    "                ('linear_regression', LinearRegression())    \n",
    "            ])\n",
    "\n",
    "    polynomial_model.fit(sample[[\"hp\"]], sample[\"mpg\"])\n",
    "    \n",
    "    name = f\"Sample {i+1}\"\n",
    "    fig.add_trace(go.Scatter(x=sample[\"hp\"], y=sample[\"mpg\"], \n",
    "        name=name, legendgroup=name, mode=\"markers\", showlegend=False,\n",
    "        marker_size=16,\n",
    "        marker_color=px.colors.qualitative.Plotly[i+1]))\n",
    "    xs = np.linspace(10, 250, 1000)\n",
    "    fig.add_trace(go.Scatter(x=xs, \n",
    "        y=polynomial_model.predict(xs[:, np.newaxis]),\n",
    "        name=name, legendgroup=name, mode=\"lines\",\n",
    "        line_color=px.colors.qualitative.Plotly[i+1]))\n",
    "\n",
    "fig.update_yaxes(range=[0, 50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:data1202] *",
   "language": "python",
   "name": "conda-env-data1202-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
