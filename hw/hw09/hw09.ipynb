{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"hw09.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 9: Text Analysis of Bloomberg Articles\n",
    "\n",
    "## Data Cleaning and EDA\n",
    "\n",
    "\n",
    "You must submit this assignment to Gradescope by the on-time deadline. **We strongly encourage you to submit your work to Gradescope several hours before the stated deadline.** This way, you will have ample time to reach out to staff for support if you encounter difficulties with submission. While course staff is happy to help guide you with submitting your assignment ahead of the deadline, we will not respond to last-minute requests for assistance.\n",
    "\n",
    "Please read the instructions carefully when submitting your work to Gradescope.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This Assignment\n",
    "\n",
    "Welcome to Homework 9! For this assignment, we will work with Bloomberg news articles on Microsoft and Microsoft stock data (MSFT).\n",
    "\n",
    "In this assignment, you will gain practice with:\n",
    "\n",
    "- Conducting data cleaning and EDA on a text-based dataset,\n",
    "- Manipulating data in `pandas` with the `datetime` and `string` accessors,\n",
    "- Writing regular expressions and using `pandas` RegEx methods, and\n",
    "- Performing sentiment analysis on text using DistilBERT.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to set up your notebook. \n",
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\")\n",
    "\n",
    "import re\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# Ensure that pandas shows at least 280 characters in columns, so we can see full articles.\n",
    "pd.set_option(\"max_colwidth\", 280)\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "sns.set()\n",
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we will use the results of the DistilBERT model, which is a Natural Language Processing (NLP) model designed to understand human language by processing text to capture the context and meaning of words within sentences. You are not expected to know the details of the model, but we will use it in this homework to perform sentiment analysis on textual data. \n",
    "\n",
    "Because of the size of the model, you will be only be given the results and not run the model yourselves.  However, instructions on how to run the model will be provided. \n",
    "\n",
    "For example, the model is part of the Hugging Face platform.  To run the model you need to have either Tensorflow, Pytorch or Flax installed. For example, with Tensorflow: \n",
    "\n",
    "```bash\n",
    "pip install tf-keras tensorflow transformers\n",
    "```\n",
    "\n",
    "Then, you can import the model: \n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "model_checkpoint = \"distilbert/distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before You Start\n",
    "\n",
    "For each question in the assignment, please write down your answer in the answer cell(s) right below the question.\n",
    "\n",
    "We understand that it is helpful to have extra cells breaking down the process towards reaching your final answer. If you happen to create new cells below your answer to run code, **NEVER** add cells between a question cell and the answer cell below it. It will cause errors when we run the autograder, and it will sometimes cause a failure to generate the PDF file.\n",
    "\n",
    "**Important note: The local autograder tests will not be comprehensive. You can pass the automated tests in your notebook but still fail tests on Gradescope after the grades are released.** Please be sure to check your results carefully.\n",
    "\n",
    "Finally, unless we state otherwise, **do not use for loops or list comprehensions**. The majority of this assignment can be done using built-in commands in `pandas` and `NumPy`.\n",
    "\n",
    "### Debugging Guide\n",
    "\n",
    "If you run into any technical issues, we highly recommend checking out the [Debugging Guide](https://mtu.instructure.com/courses/1527249/pages/debugging-guide). In this guide, you can find general questions about Jupyter notebooks / Datahub, Gradescope, and common pandas errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "<hr style=\"border: 5px solid #8a8c8c;\" />\n",
    "<hr style=\"border: 1px solid #ffcd00;\" />\n",
    "\n",
    "## Question 1: Importing the Data\n",
    "\n",
    "The data for this assignment is a subset of the financial news dataset from [this github repo](https://github.com/philipperemy/financial-news-dataset). The original datasets are no longer available online due to copyright issues, but we were allowed access for educational purposes. The data in the file `data/msft_bloomberg_news.txt` has been filtered to just Bloomberg articles published between 2010 to 2013 (inclusive) with text that contains \"Microsoft\" or \"MSFT\" (Microsoft's stock name).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 1a\n",
    "\n",
    "Let's examine the contents of the `data/msft_bloomberg_news.txt` file. Using the [`open` function](https://docs.python.org/3/library/functions.html#open) and [`read` operation](https://docs.python.org/3/tutorial/inputoutput.html#methods-of-file-objects) on a `python` file object, read **the first 1000 characters** in `data/msft_bloomberg_news.txt` and store your result in the variable `q1a`. Then, display the result so you can read it.\n",
    "\n",
    "**CAUTION: Viewing the contents of large files in a Jupyter Notebook could crash your browser. Be careful not to print the entire contents of the file.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "...\n",
    "print(q1a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### JSON\n",
    "\n",
    "The printed output you got from `q1a` is in the JSON format.\n",
    "\n",
    "- JSON stands for **JavaScript Object Notation**. It is a lightweight format for storing and transferring data.\n",
    "\n",
    "- It is:\n",
    "    - very easy for computers to read and write.\n",
    "    - moderately easy for programmers to read and write by hand.\n",
    "    - meant to be generated and parsed.\n",
    "\n",
    "- Most modern languages have an interface for working with JSON objects.\n",
    "    - JSON objects _resemble_ Python dictionaries (but are not the same!).\n",
    " \n",
    "#### JSON data types\n",
    "\n",
    "| Type | Description |\n",
    "| --- | --- |\n",
    "| String | Anything inside double quotes. |\n",
    "| Number | Any number (no difference between ints and floats). |\n",
    "| Boolean | `true` and `false`. |\n",
    "| Null | JSON's empty value, denoted by `null`. |\n",
    "| Array | Like Python lists. |\n",
    "| Object | A collection of key-value pairs, like dictionaries. Keys must be strings, values can be anything (even other objects). |\n",
    "\n",
    "See [json-schema.org](https://json-schema.org/understanding-json-schema/reference/type.html) for more details.\n",
    "\n",
    "**CAUTION: As a reminder, viewing the contents of large files in a Jupyter Notebook could crash your browser. Be careful not to print the entire contents of the file, and do not use the file explorer to open data files directly.**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 1b\n",
    "\n",
    "`pandas` has built-in readers for many different file formats, including the file format used here to store news articles. To learn more about these, check out the documentation for\n",
    "\n",
    "- `pd.read_csv` [(docs)](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html)\n",
    "- `pd.read_html`[(docs)](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_html.html)\n",
    "- `pd.read_json`[(docs)](https://pandas.pydata.org/docs/reference/api/pandas.read_json.html)\n",
    "- `pd.read_excel`[(docs)](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_excel.html).\n",
    "\n",
    "For this question, use one of these functions to:\n",
    "1. Load the file `msft_bloomberg_news.txt` in the data folder as a `DataFrame` into the variable `msft_news_df`.\n",
    "2. Set the **index** of `msft_news_df` to correspond to the `id` of each news article.\n",
    "\n",
    "\n",
    "**Hint:** If your code is taking a while to run, you may have used the incorrect data loading function for the type of the given file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "msft_news_df = ...\n",
    "msft_news_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 1c\n",
    "\n",
    "Suppose we are interested in using the news to predict future stock values. What additional data would we need to predict stock prices, and how could we connect that data to news articles? In addition, what attributes or characteristics of the news might help predict the stock value? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "\n",
    "<br/><br/>\n",
    "<hr style=\"border: 5px solid #8a8c8c;\" />\n",
    "<hr style=\"border: 1px solid #ffcd00;\" />\n",
    "\n",
    "## Question 2: Time Analysis\n",
    "\n",
    "After loading in the data, we can start exploring news articles by analyzing the relationships between the release dates (date of publication) and different topics and companies.\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 2a\n",
    "\n",
    "First, let's extract the date and time from the `released_at` column in `msft_news_df`. Notice that the date and time are encoded in the following format:\n",
    "\n",
    "```\n",
    "<date>May 29 2012</date> <time>09:40:58</time>\n",
    "<date>May 18 2011</date> <time>22:42:40</time>\n",
    "<date>August 15 2012</date> <time>00:09:02</time>\n",
    "<date>July 1 2011</date> <time>22:12:37</time>\n",
    "...\n",
    "```\n",
    "\n",
    "There are several ways to convert this to a `Timestamp` object that we can use more easily. However, for this assignment, we are going to use string manipulation functions. \n",
    "\n",
    "Create a regular expression that extracts the Month, Day, Year, Hour, Minute, and Second from the `msft_news_df[\"released_at\"]` column. You should create a new `DataFrame` called `dates` that contains:\n",
    "1. The same index as `msft_news_df` (`id`) and\n",
    "2. Column labels: `\"Month\"`, `\"Day\"`, `\"Year\"`, `\"Hour\"`, `\"Minute\"`, `\"Second\"`.\n",
    "\n",
    "Additionally, convert all numerical values (`\"Year\"`, `\"Day\"`, `\"Hour\"`, `\"Minute\"`, `\"Second\"`) to type `int`.\n",
    "\n",
    "**Hint 1:** You should use the [`Series.str.extract`](https://pandas.pydata.org/docs/reference/api/pandas.Series.str.extract.html) function.\n",
    "\n",
    "**Hint 2:** Don't forget to use raw strings and capture groups. Copy the above example text into [regex101.com](https://regex101.com/) to experiment with your regular expressions.\n",
    "\n",
    "**Hint 3:** It might be helpful to break this up into a couple of steps (e.g., first extract date values such as Month, Day, and Year and then extract time values such as Hour, Minute, and Second)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 2b\n",
    "\n",
    "Now that we've figured out how to extract dates, create a new `DataFrame` called `msft_news_2010` that only contains articles released in 2010. This `DataFrame` should contain:\n",
    "1. An index of `id` and\n",
    "2. Columns: `\"title\"`, `\"released_at\"`, `\"content\"`, `\"path\"`, `\"Month\"`, `\"Day\"`, and `\"Year\"`.\n",
    "\n",
    "**Hint:** Consider merging `msft_news_df` with `dates`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "...\n",
    "\n",
    "msft_news_2010.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 2c\n",
    "\n",
    "After processing the article release dates, we can analyze articles about different topics and companies. Note that all the articles in the provided dataset mention Microsoft/MSFT, but they can also mention other companies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each company in the list of `companies` (provided below), add a boolean column to the `msft_news_df` `DataFrame` indicating whether the corresponding company is mentioned in the text of the article. Ultimately, you should add six new columns containing `True`/`False` values to the `DataFrame`: `\"amazon\"`, `\"nintendo\"`, `\"apple\"`, `\"sony\"`, `\"facebook\"`, `\"netflix\"`. You may use a for loop over the list of companies.\n",
    "\n",
    "**Note:** Make the contents of the articles lowercase before searching for the keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "companies = [\"amazon\", \"nintendo\", \"apple\", \"sony\", \"facebook\", \"netflix\"]\n",
    "\n",
    "...\n",
    "\n",
    "msft_news_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 2d\n",
    "\n",
    "Now, we can put everything together to analyze the release dates and volume of articles for different companies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2d, Part i\n",
    "\n",
    "Create a new `DataFrame` called `year_news` that contains the number of articles mentioning each company in the list `companies` after 2010 (inclusive). `year_news` should have six columns (one column for each company), and the index of this `DataFrame` should be the release year `\"Year\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "year_news = ...\n",
    "year_news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### Question 2d, Part ii\n",
    "\n",
    "Given your code in the previous part is correct, after running the cell below, you should be able to see the number of articles released mentioning `companies` for each year. The plot should look like this:\n",
    "<center>\n",
    "<img src = \"images/num_articles.png\" width = \"500\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "for company in companies:\n",
    "    sns.lineplot(data=year_news.reset_index(),\n",
    "                 x=\"Year\",\n",
    "                 y=company,\n",
    "                 label=company)\n",
    "plt.legend(fontsize=\"12\")\n",
    "plt.xticks(np.arange(2010, 2014), np.arange(2010, 2014))\n",
    "plt.ylabel(\"Number of Articles\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.title(\"Number of Articles Released (2010-2013)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What trends do you notice in the plot above? Feel free to reference or Google any events to explain the trends seen in the graph. What are some limitations of using data and the corresponding plot to analyze the performance of different companies or trends?\n",
    "\n",
    "**Hint:** Remember the source of the articles and the subset of the articles we are analyzing in this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- END QUESTION -->\n",
    "\n",
    "<br/>\n",
    "\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Question 3: Sentiment Analysis\n",
    "\n",
    "In this section, we will continue building on our past analysis and specifically look at the **sentiment of each article** —— this will lead us to a much more direct and detailed understanding of how these articles can be used in different applications. **Sentiment analysis** is generally the computational task of classifying the emotions in a body of text as positively or negatively charged.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the results of a fine-tuned version of the **DistilBERT** model ([github](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation), [original paper](https://arxiv.org/abs/1910.01108)) to analyze the sentiment of Bloomberg news articles. DistilBERT is a neural network-based language model (a close relative to ChatGPT); we will use the model checkpoint specifically trained for sentiment analysis. These models are not in scope for DATA 2201, and we don't expect you to know how they work; take Machine Learning courses if you're interested in learning more. We are using them here to show how easy (and useful) these technologies have become.\n",
    "\n",
    "We can use the [HuggingFace library](https://huggingface.co/) to build the sentiment analysis pipeline and load the model. [Here](https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english) is the card of the model checkpoint we will use for this assignment: the model card contains general information about the model, including the base model used, training arguments, training data, etc. Again, you don't need to know this for the course but knowing about model cards is important when you start to use these techniques in your careers.\n",
    "\n",
    "The results of runnning the sentiment analysis pipeline and see examples of how we can get the sentiment for different strings is shown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**\n",
    "\n",
    "```python\n",
    "# Load the model\n",
    "sentiment_analysis = pipeline(\"sentiment-analysis\", model=model_checkpoint)\n",
    "\n",
    "# Get the sentiment of a given string\n",
    "sentiment_1 = sentiment_analysis(\"I have two dogs.\")\n",
    "print(\"Example 1: \" + str(sentiment_1))\n",
    "\n",
    "sentiment_2 = sentiment_analysis(\"I do not have dogs.\")\n",
    "print(\"Example 2: \" + str(sentiment_2))\n",
    "\n",
    "sentiment_3 = sentiment_analysis(\"Fortunately, I do not have dogs to worry about.\")\n",
    "print(\"Example 3: \" + str(sentiment_3))\n",
    "```\n",
    "\n",
    "*Output:*\n",
    "```output\n",
    "Example 1: [{'label': 'POSITIVE', 'score': 0.9955033659934998}]\n",
    "Example 2: [{'label': 'NEGATIVE', 'score': 0.9987561702728271}]\n",
    "Example 3: [{'label': 'POSITIVE', 'score': 0.9975079298019409}]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the model can determine the sentiment of phrases/sentences (not just words). The model measures the phrase's **polarity**, indicating how strongly negative or positive it is on a scale of 0 to 1.\n",
    "\n",
    "**Note:** The output is a list, and each element of the list is a dictionary with two keys (label and score). Note that we could have gotten the sentiments of the two sentences by putting them in a list (batch) and then running the pipeline once (see the code below).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "sentiments = sentiment_analysis([\"I have two dogs.\", \"I do not have dogs.\"])\n",
    "print(sentiments)\n",
    "```\n",
    "\n",
    "```output\n",
    "[{'label': 'POSITIVE', 'score': 0.9955033659934998}, {'label': 'NEGATIVE', 'score': 0.9987561702728271}]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 3a\n",
    "\n",
    "As running all the articles through the model will take a while, let's first focus on articles released in 2010. We have already filtered these articles in `q2b` and assigned them to the `DataFrame` `msft_news_2010`.\n",
    "\n",
    "Due to model input size constraints, a maximum of 512 words (tokens), and limited computational resources, we cannot load the full articles into the pipeline. Instead, we can look at the first sentence that mentions Microsoft in each article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Assign `microsoft_re` to a regular expression that captures sentences referencing \"microsoft\" or \"msft\" (in lowercase). You should assume all sentences end with `.`, `?`, or `!` and that these punctuation characters are not used for any other purpose. This is of course not true in practice (e.g., this example! and 3.14), but we will often make these simplifying assumptions to enable progress in data analysis.\n",
    "\n",
    "You should develop and test your regular expression using [regex101.com](https://regex101.com/). Here are some practice sentences.\n",
    "\n",
    "```\n",
    "have you ever worked at microsoft? i once did. microsoft is known for\n",
    "their research in ai.\n",
    "```\n",
    "\n",
    "\n",
    "Then:\n",
    "1. Canonicalize the `\"content\"` of the articles by converting the text to lowercase,\n",
    "2. Use the `microsoft_re` regular expression to extract the first sentence mentioning \"microsoft\" or \"msft\" in each article, and \n",
    "3. Create a new column `first_sentence` in `msft_news_2010` with these values. \n",
    "\n",
    "\n",
    "**Hint 1:** `Series.str.findall` function might be useful (might take around a minute to run).\n",
    "\n",
    "**Hint 2:** Consider using the negation character class `r\"[^.!?]\"`\n",
    "\n",
    "**Hint 3:** Some sentences will wrap across lines and the `.` will not match across new lines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "microsoft_re = ...\n",
    "...\n",
    "msft_news_2010.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 3b\n",
    "\n",
    "We can now turn to an alternative, more accurate way of determining the sentiment score of articles —— getting the sentiment based on the entire text, rather than getting sentiment based on the first sentence including \"microsoft\" or \"msft\" in the text. Let's load in `data/article_sentiment_logs.csv`, which contains sentiment scores of the full articles as a `DataFrame` `full_sentiments`. In this file, you are provided with logs which include the `id`, `score`, and `label` (\"N\" for \"NEGATIVE\" and \"P\" for \"POSITIVE\") in the following format: \n",
    "\n",
    "```\n",
    "<device:1> <id:77243971> <result: [0.9963290095329285 (N)]>\n",
    "<device:0> <id:14799046> <result: [0.9980687499046326 (N)]>\n",
    "<device:1> <id:43064156> <result: [0.997868537902832 (N)]>\n",
    "<device:1> <id:29402508> <result: [0.9924335479736328 (N)]>\n",
    "...\n",
    "```\n",
    "\n",
    "Run the following cell to load in the `DataFrame` and see what it contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell; no further action is needed\n",
    "full_sentiments = pd.read_csv('data/article_sentiment_logs.csv')\n",
    "full_sentiments.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the logs, modify `full_sentiments` so it ultimately just contains the `id` and `content_score` (a number ranging from -1 to 1). Then, merge this with `msft_news_2010` so we can see the results of our two methods of calculating sentiment side by side. Assign this merged `DataFrame` to `msft_scores_2010`. After the merge, make sure that only articles from 2010 appear and that the index of the `DataFrame` is the article `id`.\n",
    "\n",
    "**Note 1:** You need to negate the score of negatively classified articles (indicated by \"N\").\n",
    "\n",
    "**Note 2:** If you run into issues when merging, you may need to reset `full_sentiments` by running the above cell again.\n",
    "\n",
    "**Hint 1:** The articles have a primary key `id`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "...\n",
    "msft_scores_2010.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## You have finished Homework 9!\n",
    "\n",
    "\n",
    "\n",
    "### Submission Instructions\n",
    "\n",
    "Below, you will see a cell. Running this cell will automatically generate a zip file with your autograded answers. Submit this file to the HW 9  assignment on Gradescope. \n",
    "\n",
    "**You are responsible for ensuring your submission follows our requirements. We will not be granting regrade requests nor extensions to submissions that don't follow instructions.** If you encounter any difficulties with submission, please don't hesitate to reach out to staff prior to the deadline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(pdf=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:data1202] *",
   "language": "python",
   "name": "conda-env-data1202-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "otter": {
   "OK_FORMAT": true,
   "assignment_name": "HW09",
   "require_no_pdf_confirmation": true,
   "tests": {
    "q1a": {
     "name": "q1a",
     "points": [
      2,
      2,
      1
     ],
     "suites": [
      {
       "cases": [
        {
         "code": ">>> len(q1a) == 1000\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> q1a.startswith('[{\\\"id\\\":46243185,\\\"title\\\":\\\"Ope')\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> 'companies such as  Microsoft Corp. ' in q1a\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1b": {
     "name": "q1b",
     "points": 12,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> msft_news_df.index.name == 'id'\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> set(['title', 'released_at', 'content', 'path']) == set(msft_news_df.columns)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> msft_news_df.shape == (4635, 4)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.random.seed(100)\n>>> expected = set([10511979, 35357277, 49432195, 52444917, 70540777, 80595106, 86731456, 88502258, 91893641, 97354949])\n>>> set(np.random.choice(sorted(msft_news_df.index), replace=False, size=10)) == expected\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2a": {
     "name": "q2a",
     "points": 12,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> {'Day', 'Hour', 'Minute', 'Month', 'Second', 'Year'} == set(dates.columns)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> set(dates['Month'].unique()) == {'April', 'August', 'November', 'December', \n...                                  'February', 'May', 'October', 'September', \n...                                  'January', 'June', 'March', 'July'}\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> dates['Year'].iloc[0].dtype == int\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> dates['Day'].iloc[0].dtype == int\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> dates['Hour'].iloc[0].dtype == int\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> dates['Minute'].iloc[0].dtype == int\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> dates['Second'].iloc[0].dtype == int\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2b": {
     "name": "q2b",
     "points": [
      2,
      1,
      2
     ],
     "suites": [
      {
       "cases": [
        {
         "code": ">>> msft_news_2010.shape == (568, 7)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> msft_news_2010.index.name == 'id'\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> set(msft_news_2010.columns) == {'Day', 'Month', 'Year', 'content', \n...                                 'path', 'released_at', 'title'}\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2c": {
     "name": "q2c",
     "points": 12,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> msft_news_df.shape[0] == 4635\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> set(msft_news_df.columns) == {'amazon', 'apple', 'content', 'facebook', 'netflix', \n...                               'nintendo', 'path', 'released_at', 'sony', 'title'}\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2d": {
     "name": "q2d",
     "points": 12,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> year_news.shape == (4, 6)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> set(year_news.columns) == {'amazon', 'apple', 'facebook', 'netflix', \n...                            'nintendo', 'sony'}\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> set(year_news.index) == {2010, 2011, 2012, 2013}\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3a": {
     "name": "q3a",
     "points": [
      2,
      2,
      3,
      3
     ],
     "suites": [
      {
       "cases": [
        {
         "code": ">>> 'first_sentence' in msft_news_2010.columns\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> msft_news_2010.shape == (568, 8)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> re.findall(microsoft_re, 'the other thing. The capital of seattle is microsoft headquarters. I live near msft! The end.') == [' The capital of seattle is microsoft headquarters.', ' I live near msft!']\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> re.findall(microsoft_re, 'hello') == []\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3b": {
     "name": "q3b",
     "points": [
      2
     ],
     "suites": [
      {
       "cases": [
        {
         "code": ">>> 'content_score' in msft_scores_2010.columns\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
